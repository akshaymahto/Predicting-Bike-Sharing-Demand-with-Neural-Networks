---
title: "Predicting Bike Sharing Demand with Neural Networks"
author: "Akshay Mahto"
date: "2024-11-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(keras3)
library(tfruns)
library(ggplot2)

```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


```
1. (0.4 points) Explore the overall structure of the dataset using the “str()” function. Then,
get a summary statistics of each variable using the “summary()” function. Answer the following
questions:
– How many observations (examples) do you have in the data?
– What is the type of each variable? Categorical or Numeric?
– Is there any missing value in the data?
– Draw the histogram of the variable “count”.
```

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(caret)

bikes <- read.csv("/Users/apple/Desktop/Machine-Learning-R/Assignment 4/bike.csv")

#Explore dataset structure 
str(bikes)
sapply(bikes, class)

# Summary statistics
summary(bikes)

# Is there any missing value in the data?
sum(is.na(bikes))

# Draw the histogram of the variable “count”.
ggplot(bikes, aes(x = count)) +
  geom_histogram(binwidth = 50, fill = "purple", color = "black") +
  labs(title = "Histogram of Bike Rental Count", x = "Count", y = "Frequency")
```
```
2. (0.1 points) Remove the “registered” and “casual” variables. These are the count of registered
and casual users and together they can perfectly predict “count” so we are removing them from
the model and predict count from the other features.
```

```{r}
# Remove 'registered' and 'casual' columns
bikes <- bikes[, !(names(bikes) %in% c("registered", "casual"))]
```


```
3. (0.1 points) The “count” variable is severely right-skewed. A skewed target variable can make
a machine learning model biased. For instance, in this case, lower counts are more frequent in
the data compared to higher counts . Therefore, a machine learning model trained on this data
is less likely to successfully predict higher counts. There are different ways we can transform a
right-skewed variable to a more bell-shape distribution. Common transformations for a right-
skewed data includes log, square-root, and cube-root transformations. We are going to use
square root transformation to make the distribution of count more bell-shaped. Set
the count variable in your dataframe equal to square root of count and plot its
histogram again.
```

```{r}
# Square root transformation
bikes$count <- sqrt(bikes$count)

# Histogram after transformation
hist(bikes$count, main = "Histogram of Transformed Count", xlab = "Square Root of Count", col = "purple", breaks = 50)

```
```
4. (0.4 points) The variable of “datetime” is not useful in its current form. Convert this variable
to several variables: “year”, “month”, “day of month”, “day of week”, and “hour”. You can use
as.POSIXlt function to extract those features from “datetime”. Please see the following exam-
ple. After this, remove the original “datetime” variable after conversion by setting “datetime”
be NULL.

# Sample datetime
datetime <- as.POSIXlt("2024-11-01 12:04:17")

# Extract components as numeric values
year <- as.numeric(format(datetime, "%Y"))
month <- as.numeric(format(datetime, "%m"))
day_of_month <- as.numeric(format(datetime, "%d"))
day_of_week <- as.numeric(format(datetime, "%u"))
hour <- as.numeric(format(datetime, "%H"))
```
```{r}
bikes$datetime <- as.POSIXlt(bikes$datetime)
bikes$year <- as.numeric(format(bikes$datetime, "%Y"))
bikes$month <- as.numeric(format(bikes$datetime, "%m"))
bikes$day_of_month <- as.numeric(format(bikes$datetime, "%d"))
bikes$day_of_week <- as.numeric(format(bikes$datetime, "%u"))
bikes$hour <- as.numeric(format(bikes$datetime, "%H"))

# Remove original datetime column
bikes$datetime <- NULL

```

```
5. (0.8 points) Variables “month”, “day of week”, “hour”, and “season” are categorical but
they are also circular. This means these variables are periodic in nature. If we represent
a circular variable like “month” with numeric indices 0-11 we are implying that the distance
between month 10 (November) and month 11 (December) is much closer than the distance
between month 11(December) and month 0 (January) which is not correct. Thus, a better way
to represent these variables is to map each value into a point in a circle where the lowest value
appears next to the largest value in the circle. For instance, we can transform the “month i” by
creating “month i x” and “month i y” coordinates of the point in such the circle using sine and
cosine transformations as follows:
month i x = cos( 2π ∗ month i
max(month) )
month i y = sin( 2π ∗ month i
max(month) )
For instance, month 10 is converted to month 10 x = cos(2π ∗10/11) and month 10 y = sin(2π ∗
10/11).
Convert variables “month”, “day of week”, “hour”, and “season” to their x and y coordinates
using sine and cosine transformations as explained above. Make sure to remove the original
“month”, “day of week”, “hour”, and “season” variables after transformation by setting them as
NULL.
Note: The “day of month” variable is also technically circular but this dataset only contains
days 1-19. Therefore, we do not apply such the transformation on this variable.
```
```{r}

# Maximum values for each circular variable to normalize them in the transformation
# Define a function to apply circular transformation using sine and cosine
circular_transform <- function(data, variable, max_value) {
  # Create x and y coordinates for the circular variable
  data[[paste0(variable, "_x")]] <- cos(2 * pi * data[[variable]] / max_value)
  data[[paste0(variable, "_y")]] <- sin(2 * pi * data[[variable]] / max_value)
  
  # Remove the original variable after transformation
  data[[variable]] <- NULL
  
  return(data)
}

# Apply circular transformation on 'month' with maximum value of 12
bikes <- circular_transform(bikes, "month", 12)

# Apply circular transformation on 'day_of_week' with maximum value of 7
bikes <- circular_transform(bikes, "day_of_week", 7)

# Apply circular transformation on 'hour' with maximum value of 24
bikes <- circular_transform(bikes, "hour", 24)

# Apply circular transformation on 'season' with maximum value of 4
bikes <- circular_transform(bikes, "season", 4)

# Display the first few rows of the transformed dataset
head(bikes)

```
```{r}
# Ensure 'holiday' and 'workingday' are binary (0 or 1)
bikes$holiday <- ifelse(bikes$holiday > 0, 1, 0)
bikes$workingday <- ifelse(bikes$workingday > 0, 1, 0)

# Update "categorical_vars" with the names of all non-binary categorical variables to encode
categorical_vars <- c("weather")  # Replace with actual categorical variable names in your dataset

# Perform one-hot encoding on each categorical variable
library(dplyr)
bikes <- bikes %>%
  select(-one_of(categorical_vars)) %>%
  bind_cols(model.matrix(~.-1, bikes[categorical_vars]))

# Check the structure of bike_data to confirm successful one-hot encoding
str(bikes)
```

```
6. (0.4 points) Encode the categorical variables before training the network. One-hot encode
all the categorical variables in your dataset, see your previous assignment and code demo to
see how you can do one-hot encoding for categorical variables. Note: binary variables such as
“holiday” and “workingday” have already been converted to binary 0-1 and don’t need to be
one-hot encoded. You can check their values to ensure they are binary. If not, you can use ifelse
to convert them to be binary.
```
```{r}
library(caret)

# Verify if 'holiday' and 'workingday' are binary (0 or 1)
print(unique(bikes$holiday))
print(unique(bikes$workingday))

# Ensure 'holiday' and 'workingday' are converted to binary if necessary
bikes$holiday <- ifelse(bikes$holiday > 0, 1, 0)
bikes$workingday <- ifelse(bikes$workingday > 0, 1, 0)


weather_dummies <- model.matrix(~ weather - 1, data = bikes)

# Add the dummy variables back to the main dataset
bikes <- cbind(bikes, weather_dummies)

# Remove the original 'weather' column after encoding
bikes$weather <- NULL

set.seed(2024)

# Split the data into training and testing sets (90% for training)
train_index <- createDataPartition(bikes$count, p = 0.9, list = FALSE)
bikes_train <- bikes[train_index, ]
bikes_test <- bikes[-train_index, ]

# Check and print the sizes of the training and testing sets
print(paste("Training set size:", nrow(bikes_train)))
print(paste("Testing set size:", nrow(bikes_test)))
print(paste("Training/Test Split Ratio:", round(nrow(bikes_train) / nrow(bikes), 3)))

```

```
7. (0 point) Use set.seed(2024) to set the random seed so that I can reproduce your results.
```
```{r}
# Set seed for reproducibility
set.seed(2024)

```

```
8. (0.4 points) Use Caret’s createDataPartition method as follows to split the dataset into
“bikes train” and “bikes test” (use 90% for training and 10% for testing).
inTrain = createDataPartition(bikes$count, p=0.9, list=FALSE)
bikes_train = bikes[inTrain,]
bikes_test = bikes[-inTrain,]
where “bikes” is the name of your pre-processed data frame. The first line creates a random 90%-
10% split of data such that the distribution of the target variable “bikes$count” is preserved in
each split. The “list = FALSE” option avoids returning the data as a list. Instead, “inTrain” is
a vector of indices used to get the training and test data.
```
```{r}
# Load caret package
library(caret)

# Set random seed for reproducibility
set.seed(2024)

# Create a 90%-10% data partition for training and testing
# "bikes" is the name of your pre-processed dataset
inTrain <- createDataPartition(bikes$count, p = 0.9, list = FALSE)

# Split the dataset using the indices from createDataPartition
bikes_train <- bikes[inTrain, ]  # 90% of the data for training
bikes_test <- bikes[-inTrain, ]  # 10% of the data for testing

# Optional: Print split sizes to verify
print(paste("Training set size:", nrow(bikes_train)))
print(paste("Testing set size:", nrow(bikes_test)))
```

```
9. (0.4 points) Set.seed(2024) and further divide the “bikes train” data into 90% training and 10%
validation using Caret’s “CreateDataPartition” function. This is for the later hyper-parameter
tuning.
```
```{r}
# Set seed for reproducibility
library(caret)

# Set seed for reproducibility
set.seed(2024)

# Create index for splitting data
# Using createDataPartition for stratified sampling based on 'count' variable
split_index <- createDataPartition(
  y = bikes_train$count,  # Target variable
  p = 0.9,               # Percentage for training (90%)
  list = FALSE           # Return indices as vector, not list
)

# Split the data
training_data <- bikes_train[split_index, ]
validation_data <- bikes_train[-split_index, ]

# Verify the split proportions
train_proportion <- nrow(training_data) / nrow(bikes_train)
val_proportion <- nrow(validation_data) / nrow(bikes_train)

# Print dataset sizes and proportions
cat("Original dataset size:", nrow(bikes_train), "\n")
cat("Training dataset size:", nrow(training_data), 
    sprintf("(%.1f%%)", train_proportion * 100), "\n")
cat("Validation dataset size:", nrow(validation_data), 
    sprintf("(%.1f%%)", val_proportion * 100), "\n")

```

```
10. (0.4 points) Scale the numeric variables in the training data (except for the target variable,
“count”). Use the column means and column standard deviations from the training data to scale
both the validation and test data (see code demo of week 10). Note: You should NOT scale the
categorical variables (one-hot-encoded or binary) in the data.
```
```{r}
# Load the caret package if not already loaded
library(caret)

# Function to identify numeric columns in the dataset (excluding the target column)
identify_numeric_columns <- function(dataset, target_column = "count") {
  # Identify numeric columns, excluding the target column
  numeric_columns <- names(dataset)[sapply(dataset, is.numeric)]
  numeric_columns <- setdiff(numeric_columns, target_column)
  
  return(numeric_columns)
}

# Function to scale numeric columns based on training data's mean and standard deviation
scale_numeric_data <- function(dataset, numeric_columns, train_means = NULL, train_sds = NULL) {
  # If no training means or standard deviations are provided, calculate from the current data
  if (is.null(train_means) || is.null(train_sds)) {
    train_means <- colMeans(dataset[numeric_columns], na.rm = TRUE)
    train_sds <- apply(dataset[numeric_columns], 2, sd, na.rm = TRUE)
  }
  
  # Copy the dataset to avoid altering the original
  scaled_dataset <- dataset
  
  # Scale each numeric column based on the provided or calculated means and sds
  for (col in numeric_columns) {
    scaled_dataset[[col]] <- (dataset[[col]] - train_means[col]) / train_sds[col]
  }
  
  # Return the scaled dataset and the scaling parameters used
  return(list(
    scaled_data = scaled_dataset,
    means = train_means,
    sds = train_sds
  ))
}

# Example usage
# Get list of numeric columns (excluding 'count')
numeric_columns <- identify_numeric_columns(bikes)

# Scale the dataset and retain the mean and sd from the training set
scaled_result <- scale_numeric_data(bikes, numeric_columns)
scaled_data <- scaled_result$scaled_data
train_means <- scaled_result$means
train_sds <- scaled_result$sds
```

```
11. (1.6 points) In this part, we want to build a two-hidden layer neural network to predict the
total counts of bike rentals, fine-tune the hyper-parameters of this model, and find the best set
of hyper-parameters that can give us the best validation performance.
Specifically, we create an R script and name it “bike model.R”. In this script, we define a set of
flags for hyper-parameters we want to tune, including
– the number of nodes in hidden layer 1
– the number of nodes in hidden layer 2
– the activation functions
– the batch size
– the learning rate
Then, we build, compile, and fit the model as usual. You might want to see a concrete example
in the code demo of week 10 (“fashion mnist.R”).
After setting this script, go back to your original R notebook, run “tuning run” from “tfruns”
package to fine-tune the above hyper-parameters. As for the candidate values of your hyper-
parameters, you can determine based on your preference, and you can check one concrete example
in the code demo of week 10.
Finally, print the returned value from “tuning run” to see the metrics for each run. Which run
(which hyper-parameter combination) gives the best mean squared error on the validation set?
And print the learning curve for your best model. You can just take a screenshot of the learning
curve of your best model and submit it with the rest of your files.
Note: The “fit” function in keras does not accept a dataframe and only takes a matrix. If you
want to pass a dataframe as training or validation data to the fit function, you must first use
“as.matrix” function to convert it to matrix before passing it to the fit function; for example,
“as.matrix(your training dataframe)” or “as.matrix(your validation dataframe)”.
```

```{r}
library(tfruns)

# fine tuning the hyper-parameters
runs <- tuning_run("bike_model.R",
  flags = list(
    nodes1 = c(32, 64, 128),
    nodes2 = c(16, 32, 64),
    activation = c("relu", "tanh", "sigmoid"),
    batch_size = c(32, 64),
    learning_rate = c(0.001, 0.0001)
  ),
  sample = 0.05
)

runs= runs[order(runs$metric_val_loss),]
print(runs)

view_run(runs$run_dir[1])
```
```
12. (0.4 points) Measure the performance of your best model (after tuning) on the test set and
compute its RMSE. Note that you must reverse the square root transformation you did in q3
by taking the square of the predictions returned by the neural network model and compare it to
the original count value (taking the square of the test target values). Doing this helps us get the
RMSE in the original scale.
```

```{r}
library(keras3)
source("bike_model.R")

# Assuming you have already loaded your test data
# If not, load it similarly to how you loaded training and validation data
x_test <- as.matrix(training_data[, !names(training_data) %in% c("count")])
y_test <- as.matrix(training_data$count)

# Convert test data to numeric format
x_test <- matrix(as.numeric(x_test), nrow = nrow(x_test))
y_test <- matrix(as.numeric(y_test), nrow = nrow(y_test))

# Make predictions on the test set
# y_pred <- model %>% predict(x_test)
y_pred <- predict(model, x_test)

# Reverse the square root transformation
y_pred_reversed <- y_pred^2
y_test_reversed <- y_test^2

# Calculate RMSE
rmse <- sqrt(mean((y_test_reversed - y_pred_reversed)^2))
print(paste("RMSE for the nn model on test set (original scale):", round(rmse, 2)))

cat("RMSE on the test set (original scale):", rmse, "\n")

# Optionally, you can also use the built-in evaluation function
eval_results <- model %>% evaluate(x_test, y_test)
print(paste("Mean Squared Error on test set:", eval_results["mean_squared_error"]))
```



```
13. (0.6 points) Use a simple linear regression model to predict the count. Train and test your
model on the same data you used to train and test your best neural network model. Compare
the RMSE of the linear model on the test data with the RMSE of the neural network model.
How does your neural network model compare to a simple linear model? Note that you need to
reverse the square root transformation again in this case like what you did in q12.
```
```{r}
        # Set the seed for reproducibility
set.seed(2024)

# Train the simple linear regression model
linear_model <- lm(count ~ ., data = bikes_train)  # Use all features in bikes_train for prediction

# Check the summary of the model
summary(linear_model)

# Make predictions using the linear regression model on the test set
linear_predictions <- predict(linear_model, bikes_test)

# Reverse the square root transformation for the linear model predictions
linear_predictions_original_scale <- linear_predictions^2

# Reverse the square root transformation for the actual count values in the test set
actual_values_original_scale <- bikes_test$count^2

# Calculate RMSE for the linear regression model on the original scale
linear_rmse <- sqrt(mean((linear_predictions_original_scale - actual_values_original_scale)^2))
 
# Print the RMSE for the linear model
print(paste("RMSE for the linear model on test set (original scale):", round(linear_rmse, 2)))

# Assuming 'neural_net_rmse' holds the RMSE for the neural network model
# Print the RMSE for the neural network model (assuming you have already computed this from the previous steps)
print(paste("RMSE for the neural network model on test set (original scale):", round(rmse, 2)))

# Compare RMSE values between the linear model and the neural network model
if (linear_rmse < rmse) {
  print("The linear model performed better than the neural network model.")
} else {
  print("The neural network model performed better than the linear model.")
}

```


